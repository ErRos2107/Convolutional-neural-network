{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below a skeleton class and associated test functions for the `fprop`, `bprop` and `grads_wrt_params` methods of the ConvolutionalLayer class are included.\n",
    "\n",
    "The test functions assume that in your implementation of `fprop` for the convolutional layer, outputs are calculated only for 'valid' overlaps of the kernel filters with the input - i.e. without any padding.\n",
    "\n",
    "It is also assumed that if convolutions with non-unit strides are implemented the default behaviour is to take unit-strides, with the test cases only correct for unit strides in both directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three test functions are defined in the cell below. All the functions take as first argument the *class* corresponding to the convolutional layer implementation to be tested (**not** an instance of the class). It is assumed the class being tested has an `__init__` method with at least all of the arguments defined in the skeleton definition above. A boolean second argument to each function can be used to specify if the layer implements a cross-correlation or convolution based operation (see note in [seventh lecture slides](http://www.inf.ed.ac.uk/teaching/courses/mlp/2016/mlp07-cnn.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "\n",
    "def test_conv_layer_fprop(layer_class, do_cross_correlation=False):\n",
    "    \"\"\"Tests `fprop` method of a convolutional layer.\n",
    "    \n",
    "    Checks the outputs of `fprop` method for a fixed input against known\n",
    "    reference values for the outputs and raises an AssertionError if\n",
    "    the outputted values are not consistent with the reference values. If\n",
    "    tests are all passed returns True.\n",
    "    \n",
    "    Args:\n",
    "        layer_class: Convolutional layer implementation following the \n",
    "            interface defined in the provided skeleton class.\n",
    "        do_cross_correlation: Whether the layer implements an operation\n",
    "            corresponding to cross-correlation (True) i.e kernels are\n",
    "            not flipped before sliding over inputs, or convolution\n",
    "            (False) with filters being flipped.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: Raised if output of `layer.fprop` is inconsistent \n",
    "            with reference values either in shape or values.\n",
    "    \"\"\"\n",
    "    inputs = np.arange(96).reshape((2, 3, 4, 4))\n",
    "    kernels = np.arange(-12, 12).reshape((2, 3, 2, 2))\n",
    "    if do_cross_correlation:\n",
    "        kernels = kernels[:, :, ::-1, ::-1]\n",
    "    biases = np.arange(2)\n",
    "    true_output = np.array(\n",
    "        [[[[ -958., -1036., -1114.],\n",
    "           [-1270., -1348., -1426.],\n",
    "           [-1582., -1660., -1738.]],\n",
    "          [[ 1707.,  1773.,  1839.],\n",
    "           [ 1971.,  2037.,  2103.],\n",
    "           [ 2235.,  2301.,  2367.]]],\n",
    "         [[[-4702., -4780., -4858.],\n",
    "           [-5014., -5092., -5170.],\n",
    "           [-5326., -5404., -5482.]],\n",
    "          [[ 4875.,  4941.,  5007.],\n",
    "           [ 5139.,  5205.,  5271.],\n",
    "           [ 5403.,  5469.,  5535.]]]]\n",
    "    )\n",
    "    \n",
    "    layer = layer_class(\n",
    "        num_input_channels=kernels.shape[1], \n",
    "        num_output_channels=kernels.shape[0], \n",
    "        input_dim_1=inputs.shape[2], \n",
    "        input_dim_2=inputs.shape[3],\n",
    "        kernel_dim_1=kernels.shape[2],\n",
    "        kernel_dim_2=kernels.shape[3]\n",
    "    )\n",
    "    layer.params = [kernels, biases]\n",
    "    layer_output = layer.fprop(inputs)\n",
    "    \n",
    "    assert layer_output.shape == true_output.shape, (\n",
    "        'Layer fprop gives incorrect shaped output. '\n",
    "        'Correct shape is \\n\\n{0}\\n\\n but returned shape is \\n\\n{1}.'\n",
    "        .format(true_output.shape, layer_output.shape)\n",
    "    )\n",
    "    assert np.allclose(layer_output, true_output), (\n",
    "        'Layer fprop does not give correct output. '\n",
    "        'Correct output is \\n\\n{0}\\n\\n but returned output is \\n\\n{1}\\n\\n difference is \\n\\n{2}.'\n",
    "        .format(true_output, layer_output, true_output-layer_output)\n",
    "    )\n",
    "    return True\n",
    "\n",
    "def test_conv_layer_bprop(layer_class, do_cross_correlation=False):\n",
    "    \"\"\"Tests `bprop` method of a convolutional layer.\n",
    "    \n",
    "    Checks the outputs of `bprop` method for a fixed input against known\n",
    "    reference values for the gradients with respect to inputs and raises \n",
    "    an AssertionError if the returned values are not consistent with the\n",
    "    reference values. If tests are all passed returns True.\n",
    "    \n",
    "    Args:\n",
    "        layer_class: Convolutional layer implementation following the \n",
    "            interface defined in the provided skeleton class.\n",
    "        do_cross_correlation: Whether the layer implements an operation\n",
    "            corresponding to cross-correlation (True) i.e kernels are\n",
    "            not flipped before sliding over inputs, or convolution\n",
    "            (False) with filters being flipped.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: Raised if output of `layer.bprop` is inconsistent \n",
    "            with reference values either in shape or values.\n",
    "    \"\"\"\n",
    "    inputs = np.arange(96).reshape((2, 3, 4, 4))\n",
    "    kernels = np.arange(-12, 12).reshape((2, 3, 2, 2))\n",
    "    if do_cross_correlation:\n",
    "        kernels = kernels[:, :, ::-1, ::-1]\n",
    "    biases = np.arange(2)\n",
    "    grads_wrt_outputs = np.arange(-20, 16).reshape((2, 2, 3, 3))\n",
    "    outputs = np.array(\n",
    "        [[[[ -958., -1036., -1114.],\n",
    "           [-1270., -1348., -1426.],\n",
    "           [-1582., -1660., -1738.]],\n",
    "          [[ 1707.,  1773.,  1839.],\n",
    "           [ 1971.,  2037.,  2103.],\n",
    "           [ 2235.,  2301.,  2367.]]],\n",
    "         [[[-4702., -4780., -4858.],\n",
    "           [-5014., -5092., -5170.],\n",
    "           [-5326., -5404., -5482.]],\n",
    "          [[ 4875.,  4941.,  5007.],\n",
    "           [ 5139.,  5205.,  5271.],\n",
    "           [ 5403.,  5469.,  5535.]]]]\n",
    "    )\n",
    "    true_grads_wrt_inputs = np.array(\n",
    "      [[[[ 147.,  319.,  305.,  162.],\n",
    "         [ 338.,  716.,  680.,  354.],\n",
    "         [ 290.,  608.,  572.,  294.],\n",
    "         [ 149.,  307.,  285.,  144.]],\n",
    "        [[  23.,   79.,   81.,   54.],\n",
    "         [ 114.,  284.,  280.,  162.],\n",
    "         [ 114.,  272.,  268.,  150.],\n",
    "         [  73.,  163.,  157.,   84.]],\n",
    "        [[-101., -161., -143.,  -54.],\n",
    "         [-110., -148., -120.,  -30.],\n",
    "         [ -62.,  -64.,  -36.,    6.],\n",
    "         [  -3.,   19.,   29.,   24.]]],\n",
    "       [[[  39.,   67.,   53.,   18.],\n",
    "         [  50.,   68.,   32.,   -6.],\n",
    "         [   2.,  -40.,  -76.,  -66.],\n",
    "         [ -31.,  -89., -111.,  -72.]],\n",
    "        [[  59.,  115.,  117.,   54.],\n",
    "         [ 114.,  212.,  208.,   90.],\n",
    "         [ 114.,  200.,  196.,   78.],\n",
    "         [  37.,   55.,   49.,   12.]],\n",
    "        [[  79.,  163.,  181.,   90.],\n",
    "         [ 178.,  356.,  384.,  186.],\n",
    "         [ 226.,  440.,  468.,  222.],\n",
    "         [ 105.,  199.,  209.,   96.]]]])\n",
    "    layer = layer_class(\n",
    "        num_input_channels=kernels.shape[1], \n",
    "        num_output_channels=kernels.shape[0], \n",
    "        input_dim_1=inputs.shape[2], \n",
    "        input_dim_2=inputs.shape[3],\n",
    "        kernel_dim_1=kernels.shape[2],\n",
    "        kernel_dim_2=kernels.shape[3]\n",
    "    )\n",
    "    layer.params = [kernels, biases]\n",
    "    layer_grads_wrt_inputs = layer.bprop(inputs, outputs, grads_wrt_outputs)\n",
    "    assert layer_grads_wrt_inputs.shape == true_grads_wrt_inputs.shape, (\n",
    "        'Layer bprop returns incorrect shaped array. '\n",
    "        'Correct shape is \\n\\n{0}\\n\\n but returned shape is \\n\\n{1}.'\n",
    "        .format(true_grads_wrt_inputs.shape, layer_grads_wrt_inputs.shape)\n",
    "    )\n",
    "    assert np.allclose(layer_grads_wrt_inputs, true_grads_wrt_inputs), (\n",
    "        'Layer bprop does not return correct values. '\n",
    "        'Correct output is \\n\\n{0}\\n\\n but returned output is \\n\\n{1}\\n\\n difference is \\n\\n{2}'\n",
    "        .format(true_grads_wrt_inputs, layer_grads_wrt_inputs, layer_grads_wrt_inputs-true_grads_wrt_inputs)\n",
    "    )\n",
    "    return True\n",
    "\n",
    "def test_conv_layer_grad_wrt_params(\n",
    "        layer_class, do_cross_correlation=False):\n",
    "    \"\"\"Tests `grad_wrt_params` method of a convolutional layer.\n",
    "    \n",
    "    Checks the outputs of `grad_wrt_params` method for fixed inputs \n",
    "    against known reference values for the gradients with respect to \n",
    "    kernels and biases, and raises an AssertionError if the returned\n",
    "    values are not consistent with the reference values. If tests\n",
    "    are all passed returns True.\n",
    "    \n",
    "    Args:\n",
    "        layer_class: Convolutional layer implementation following the \n",
    "            interface defined in the provided skeleton class.\n",
    "        do_cross_correlation: Whether the layer implements an operation\n",
    "            corresponding to cross-correlation (True) i.e kernels are\n",
    "            not flipped before sliding over inputs, or convolution\n",
    "            (False) with filters being flipped.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: Raised if output of `layer.bprop` is inconsistent \n",
    "            with reference values either in shape or values.\n",
    "    \"\"\"\n",
    "    inputs = np.arange(96).reshape((2, 3, 4, 4))\n",
    "    kernels = np.arange(-12, 12).reshape((2, 3, 2, 2))\n",
    "    biases = np.arange(2)\n",
    "    grads_wrt_outputs = np.arange(-20, 16).reshape((2, 2, 3, 3))\n",
    "    true_kernel_grads = np.array(\n",
    "        [[[[ -240.,  -114.],\n",
    "         [  264.,   390.]],\n",
    "        [[-2256., -2130.],\n",
    "         [-1752., -1626.]],\n",
    "        [[-4272., -4146.],\n",
    "         [-3768., -3642.]]],\n",
    "       [[[ 5268.,  5232.],\n",
    "         [ 5124.,  5088.]],\n",
    "        [[ 5844.,  5808.],\n",
    "         [ 5700.,  5664.]],\n",
    "        [[ 6420.,  6384.],\n",
    "         [ 6276.,  6240.]]]])\n",
    "    if do_cross_correlation:\n",
    "        kernels = kernels[:, :, ::-1, ::-1]\n",
    "        true_kernel_grads = true_kernel_grads[:, :, ::-1, ::-1]\n",
    "    true_bias_grads = np.array([-126.,   36.])\n",
    "    layer = layer_class(\n",
    "        num_input_channels=kernels.shape[1], \n",
    "        num_output_channels=kernels.shape[0], \n",
    "        input_dim_1=inputs.shape[2], \n",
    "        input_dim_2=inputs.shape[3],\n",
    "        kernel_dim_1=kernels.shape[2],\n",
    "        kernel_dim_2=kernels.shape[3]\n",
    "    )\n",
    "    layer.params = [kernels, biases]\n",
    "    layer_kernel_grads, layer_bias_grads = (\n",
    "        layer.grads_wrt_params(inputs, grads_wrt_outputs))\n",
    "    assert layer_kernel_grads.shape == true_kernel_grads.shape, (\n",
    "        'grads_wrt_params gives incorrect shaped kernel gradients output. '\n",
    "        'Correct shape is \\n\\n{0}\\n\\n but returned shape is \\n\\n{1}.'\n",
    "        .format(true_kernel_grads.shape, layer_kernel_grads.shape)\n",
    "    )\n",
    "    assert np.allclose(layer_kernel_grads, true_kernel_grads), (\n",
    "        'grads_wrt_params does not give correct kernel gradients output. '\n",
    "        'Correct output is \\n\\n{0}\\n\\n but returned output is \\n\\n{1}.'\n",
    "        .format(true_kernel_grads, layer_kernel_grads)\n",
    "    )\n",
    "    assert layer_bias_grads.shape == true_bias_grads.shape, (\n",
    "        'grads_wrt_params gives incorrect shaped bias gradients output. '\n",
    "        'Correct shape is \\n\\n{0}\\n\\n but returned shape is \\n\\n{1}.'\n",
    "        .format(true_bias_grads.shape, layer_bias_grads.shape)\n",
    "    )\n",
    "    assert np.allclose(layer_bias_grads, true_bias_grads), (\n",
    "        'grads_wrt_params does not give correct bias gradients output. '\n",
    "        'Correct output is \\n\\n{0}\\n\\n but returned output is \\n\\n{1}.'\n",
    "        .format(true_bias_grads, layer_bias_grads)\n",
    "    )\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of using the test functions if given in the cell below. This assumes you implement a convolution (rather than cross-correlation) operation. If the implementation is correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "grads_wrt_params does not give correct kernel gradients output. Correct output is \n\n[[[[ -240.  -114.]\n   [  264.   390.]]\n\n  [[-2256. -2130.]\n   [-1752. -1626.]]\n\n  [[-4272. -4146.]\n   [-3768. -3642.]]]\n\n\n [[[ 5268.  5232.]\n   [ 5124.  5088.]]\n\n  [[ 5844.  5808.]\n   [ 5700.  5664.]]\n\n  [[ 6420.  6384.]\n   [ 6276.  6240.]]]]\n\n but returned output is \n\n[[[[  390.   264.]\n   [ -114.  -240.]]\n\n  [[-1626. -1752.]\n   [-2130. -2256.]]\n\n  [[-3642. -3768.]\n   [-4146. -4272.]]]\n\n\n [[[ 5088.  5124.]\n   [ 5232.  5268.]]\n\n  [[ 5664.  5700.]\n   [ 5808.  5844.]]\n\n  [[ 6240.  6276.]\n   [ 6384.  6420.]]]].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-543cc148b211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfprop_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_conv_layer_fprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvolutionalLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbprop_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_conv_layer_bprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvolutionalLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrads_wrt_param_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_conv_layer_grad_wrt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvolutionalLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfprop_correct\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgrads_wrt_param_correct\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbprop_correct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'All tests passed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-5d615749bea7>\u001b[0m in \u001b[0;36mtest_conv_layer_grad_wrt_params\u001b[0;34m(layer_class, do_cross_correlation)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;34m'grads_wrt_params does not give correct kernel gradients output. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m'Correct output is \\n\\n{0}\\n\\n but returned output is \\n\\n{1}.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_kernel_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_kernel_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m     assert layer_bias_grads.shape == true_bias_grads.shape, (\n",
      "\u001b[0;31mAssertionError\u001b[0m: grads_wrt_params does not give correct kernel gradients output. Correct output is \n\n[[[[ -240.  -114.]\n   [  264.   390.]]\n\n  [[-2256. -2130.]\n   [-1752. -1626.]]\n\n  [[-4272. -4146.]\n   [-3768. -3642.]]]\n\n\n [[[ 5268.  5232.]\n   [ 5124.  5088.]]\n\n  [[ 5844.  5808.]\n   [ 5700.  5664.]]\n\n  [[ 6420.  6384.]\n   [ 6276.  6240.]]]]\n\n but returned output is \n\n[[[[  390.   264.]\n   [ -114.  -240.]]\n\n  [[-1626. -1752.]\n   [-2130. -2256.]]\n\n  [[-3642. -3768.]\n   [-4146. -4272.]]]\n\n\n [[[ 5088.  5124.]\n   [ 5232.  5268.]]\n\n  [[ 5664.  5700.]\n   [ 5808.  5844.]]\n\n  [[ 6240.  6276.]\n   [ 6384.  6420.]]]]."
     ]
    }
   ],
   "source": [
    "from mlp.layers import ConvolutionalLayer\n",
    "fprop_correct = test_conv_layer_fprop(ConvolutionalLayer, False)\n",
    "bprop_correct = test_conv_layer_bprop(ConvolutionalLayer, False)\n",
    "grads_wrt_param_correct = test_conv_layer_grad_wrt_params(ConvolutionalLayer, False)\n",
    "if fprop_correct and grads_wrt_param_correct and bprop_correct:\n",
    "    print('All tests passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Created `%t` as an alias for `%timeit`.\n",
      "Created `%%t` as an alias for `%%timeit`.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%alias_magic t timeit\n",
    "from scipy.signal import convolve2d\n",
    "import numpy as np\n",
    "from mlp.layers import MaxPoolingLayer, ReshapeLayer,ConvolutionalLayer\n",
    "#inputs = np.arange(96).reshape((2, 3, 4, 4))\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "\n",
    "\n",
    "grads_wrt_outputs = np.arange(-20, 4).reshape((2, 3, 2, 2))\n",
    "layer = MaxPoolingLayer()\n",
    "def test():\n",
    "    x_shape = (2, 3, 4, 4)\n",
    "    inputs = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "    out =  layer.fprop(inputs)\n",
    "    #return layer.bprop_fast(inputs, outputs, grads_wrt_outputs)\n",
    "    correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "    print('Testing max_pool_forward_naive function:')\n",
    "    print(out)\n",
    "    #print( 'difference: ', rel_error(out, correct_out))\n",
    "\n",
    "def test_naive():\n",
    "\n",
    "    return layer.fprop_naive(inputs)    \n",
    "    #return layer.bprop(inputs, outputs, grads_wrt_outputs)\n",
    "\n",
    "def test_bprop():\n",
    "    inputs = np.random.randn(3, 2, 8, 8)\n",
    "    dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_naive(x, pool_param)[0], x, dout)\n",
    "\n",
    "    dout = np.random.randn(3, 2, 4, 4)\n",
    "    outputs = layer.fprop(inputs)\n",
    "    out = layer.bprop(inputs, outputs, dout)\n",
    "\n",
    "def c():\n",
    "    inputs = np.random.randn(100, 1, 28, 28)\n",
    "    kernels = np.random.randn(5, 1, 2, 2)\n",
    "    Clayer = ConvolutionalLayer(num_input_channels=kernels.shape[1], \n",
    "        num_output_channels=kernels.shape[0], \n",
    "        input_dim_1=inputs.shape[2], \n",
    "        input_dim_2=inputs.shape[3],\n",
    "        kernel_dim_1=kernels.shape[2],\n",
    "        kernel_dim_2=kernels.shape[3])\n",
    "    output = Clayer.fprop(inputs)\n",
    "    da = np.ones_like(output)\n",
    "    din = Clayer.bprop(inputs,output, da)\n",
    "    \n",
    "    return Clayer.grads_wrt_params(inputs,da)\n",
    "\n",
    "def c_f():\n",
    "    inputs = np.random.randn(2, 1, 4, 4)\n",
    "    kernels = np.random.randn(1, 1, 2, 2)\n",
    "    Clayer = ConvolutionalLayer(num_input_channels=kernels.shape[1], \n",
    "        num_output_channels=kernels.shape[0], \n",
    "        input_dim_1=inputs.shape[2], \n",
    "        input_dim_2=inputs.shape[3],\n",
    "        kernel_dim_1=kernels.shape[2],\n",
    "        kernel_dim_2=kernels.shape[3])\n",
    "    output1 = Clayer.fprop(inputs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Clayer.bprop(inputs,output1, np.ones_like(output1))\n",
    "\n",
    "def c_con2d():\n",
    "    inputs = np.random.randn(2, 1, 4, 4)\n",
    "    kernels = np.random.randn(1, 1, 2, 2)\n",
    "    \n",
    "    output2 = []\n",
    "    for i in range(inputs.shape[0]):\n",
    "        \n",
    "        output2.append(convolve2d(inputs[i,0,...], kernels[0,0,...],mode='valid'))\n",
    "    return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%t c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "(4, 4) (4, 4)\n",
      "983 µs ± 132 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%t c_f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.9 µs ± 7.02 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%t c_con2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
    "\n",
    "out, _ = max_pool_forward_naive(x, pool_param)\n",
    "\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "# Compare your output with ours. Difference should be around 1e-8.\n",
    "print 'Testing max_pool_forward_naive function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "inputs = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "out = layer.fprop(inputs)\n",
    "\n",
    "\n",
    "ReshapeLayer((3,4,4)).fprop(out).shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 ms ± 101 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%t test_naive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 2, 2)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape\n",
    "#inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 63])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(inputs[:,0,:,:],axis=(-2,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\mlp\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=inputs[:,0,:,:] \n",
    "print(x.shape)\n",
    "mask = x == np.amax(x,axis=(-2,-1))\n",
    "mask #* grads_wrt_outputs[:, 0, 0,0][..., None,None]\n",
    "#(x[:,...]== np.amax(x[:,...],axis=(-2,-1))[...,None])\n",
    "#np.ma.masked_where(x== np.amax(x,axis=(-2,-1))[...,None],x)\n",
    "#np.amax(x,axis=(-2,-1))[...,None].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\mlp\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask=np.amax(inputs[:,0,:,:],axis=(-2,-1))[...,None]\n",
    "x==mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[16, 17, 18, 19],\n",
       "         [20, 21, 22, 23],\n",
       "         [24, 25, 26, 27],\n",
       "         [28, 29, 30, 31]],\n",
       "\n",
       "        [[32, 33, 34, 35],\n",
       "         [36, 37, 38, 39],\n",
       "         [40, 41, 42, 43],\n",
       "         [44, 45, 46, 47]]],\n",
       "\n",
       "\n",
       "       [[[48, 49, 50, 51],\n",
       "         [52, 53, 54, 55],\n",
       "         [56, 57, 58, 59],\n",
       "         [60, 61, 62, 63]],\n",
       "\n",
       "        [[64, 65, 66, 67],\n",
       "         [68, 69, 70, 71],\n",
       "         [72, 73, 74, 75],\n",
       "         [76, 77, 78, 79]],\n",
       "\n",
       "        [[80, 81, 82, 83],\n",
       "         [84, 85, 86, 87],\n",
       "         [88, 89, 90, 91],\n",
       "         [92, 93, 94, 95]]]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.40947002, -0.51684908,  0.        ],\n",
       "         [ 0.        ,  0.61040263, -0.09915074,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  1.03016679, -0.77273821,  0.        ],\n",
       "         [ 0.        , -1.61940659,  0.11154493,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.03185187,  0.11020939,  0.        ],\n",
       "         [ 0.        , -1.56425638,  0.56492005,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.62139328,  1.01756094,  0.        ],\n",
       "         [ 0.        , -0.48033081,  0.14576107,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        , -0.03109896,  1.13358956,  0.        ],\n",
       "         [ 0.        , -0.93122643,  2.08224488,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        , -0.39894531, -1.16137009,  0.        ],\n",
       "         [ 0.        ,  0.08940904, -1.13225137,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.random.randn(2, 3, 2, 2)\n",
    "pad=1\n",
    "y = np.pad(inputs, ((0,), (0,), (pad,), (pad,)), 'constant')\n",
    "#(batch_size, num_input_channels, input_dim_1, input_dim_2).\n",
    "dinputs = np.zeros(inputs.shape)\n",
    "\n",
    "        # Pad the grads_wrt_outputs\n",
    "dinputs_pad = np.pad(inputs, ((0,0),(0,0), (pad,pad), (pad,pad)),'constant')\n",
    "x.shape\n",
    "dinputs_pad"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
